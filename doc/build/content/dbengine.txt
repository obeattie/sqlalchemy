Database Engines {@name=dbengine}
============================

A database engine is a subclass of `sqlalchemy.sql.Engine`, and is the starting point for where SQLAlchemy provides a layer of abstraction on top of the various DBAPI2 database modules.  For all databases supported by SA, there is a specific "implementation" module, found in the `sqlalchemy.databases` package, that provides all the objects an `Engine` needs in order to perform its job.  A typical user of SQLAlchemy never needs to deal with these modules directly.  For many purposes, the only knowledge that's needed is how to create an Engine for a particular connection URL.  When dealing with direct execution of SQL statements, one would also be aware of Result, Connection, and Transaction objects.  The primary public facing objects are:

* **URL** - represents the identifier for a particular database.  URL objects are usually created automatically based on a given connect string passed to the `create_engine()` function.
* **Engine** - Combines a connection-providing resource with implementation-provided objects that know how to generate, execute, and gather information about SQL statements.  It also provides the primary interface by which Connections are obtained, as well as a context for constructed SQL objects and schema constructs to "implicitly execute" themselves, which is an optional feature of SA 0.2.  The Engine object that is normally dealt with is an instance of `sqlalchemy.engine.base.ComposedSQLEngine`.
* **Connection** - represents a connection to the database.  The underlying connection object returned by a DBAPI's connect() method is referenced internally by the Connection object.  Connection provides methods that handle the execution of SQLAlchemy's own SQL construct objects, as well as literal string-based statements.  
* **Transaction** - represents a transaction on a single Connection.  Includes `begin()`, `commit()` and `rollback()` methods that support basic "nestable" behavior, meaning an outermost transaction is maintained against multiple nested calls to begin/commit.
* **ResultProxy** - Represents the results of an execution, and is most analgous to the cursor object in DBAPI.  It primarily allows iteration over result sets, but also provides an interface to information about inserts/updates/deletes, such as the count of rows affected, last inserted IDs, etc.
* **RowProxy** -  Represents a single row returned by the fetchone() method on ResultProxy.

Underneath the public-facing API of `ComposedSQLEngine`, several components are provided by database implementations to provide the full behavior, including:

* **Dialect** - this object is provided by database implementations to describe the behavior of a particular database.  It acts as a repository for metadata about a database's characteristics, and provides factory methods for other objects that deal with generating SQL strings and objects that handle some of the details of statement execution.  
* **ConnectionProvider** - this object knows how to return a DBAPI connection object.  It typically talks to a connection pool.
* **ExecutionContext** - this object is created for each execution of a single SQL statement, and stores information such as the last primary keys inserted, the total count of rows affected, etc.  It also may implement any special logic that various DBAPI modules may require before or after a statement execution.
* **Compiler** - receives SQL expression objects and assembles them into strings that are suitable for direct execution, as well as collecting bind parameters into a dictionary or list to be sent along with the statement.
* **SchemaGenerator** - receives collections of Schema objects and knows how to generate the appropriate SQL for `CREATE` and `DROP` statements.

### Supported Databases {@name=supported}

Engines exist for SQLite, Postgres, MySQL, MS-SQL, and Oracle, using the Pysqlite, Psycopg (1 or 2), MySQLDB, adodbapi or pymssql, and cx_Oracle modules.  There is also not-well tested support for Firebird.   For each engine, a distinct Python module exists in the `sqlalchemy.databases` package, which provides implementations of some of the objects mentioned in the previous section.

### Establishing a Database Engine {@name=establishing}

SQLAlchemy 0.2 indicates the source of an Engine strictly via [RFC-1738](http://rfc.net/rfc1738.html) style URLs, combined with optional keyword arguments to specify options for the Engine.  The form of the URL is:

    driver://username:password@host:port/database

Available drivernames are `sqlite`, `mysql`, `postgres`, `oracle`, `mssql`, and `firebird`.  For sqlite, the database name is the filename to connect to, or the special name ":memory:" which indicates an in-memory database.  The URL is typically sent as a string to the `create_engine()` function:

    db = create_engine('postgres://scott:tiger@localhost:5432/mydatabase')
    sqlite_db = create_engine('sqlite:///mydb.txt')
    mysql_db = create_engine('sqlite://localhost/foo')
    oracle_db = create_engine('oracle://scott:tiger@dsn')

### Database Engine Options {@name=options}

Keyword options can also be specified to `create_engine()`, following the string URL as follows:

    db = create_engine('postgres://...', encoding='latin1', echo=True, module=psycopg1)

* pool=None : an instance of `sqlalchemy.pool.Pool` to be used as the underlying source for connections, overriding the engine's connect arguments (pooling is described in [pool](rel:pool)).  If None, a default `Pool` (usually `QueuePool`, or `SingletonThreadPool` in the case of SQLite) will be created using the engine's connect arguments.

Example:

        {python}
        from sqlalchemy import *
        import sqlalchemy.pool as pool
        import MySQLdb

        def getconn():
            return MySQLdb.connect(user='ed', dbname='mydb')

        engine = create_engine('mysql', pool=pool.QueuePool(getconn, pool_size=20, max_overflow=40))

* echo=False : if True, the Engine will log all statements as well as a repr() of their parameter lists to the engines logger, which defaults to sys.stdout.  A SQLEngine instances' "echo" data member can be modified at any time to turn logging on and off.  If set to the string 'debug', result rows will be printed to the standard output as well.
* logger=None : a file-like object where logging output can be sent, if echo is set to True.  This defaults to sys.stdout.
* module=None : used by Oracle and Postgres, this is a reference to a DBAPI2 module to be used instead of the engine's default module.  For Postgres, the default is psycopg2, or psycopg1 if 2 cannot be found.  For Oracle, its cx_Oracle.
* default_ordering=False : if True, table objects and associated joins and aliases will generate information used for ordering by primary keys (or OIDs, if the database supports OIDs).  This information is used by the Mapper system to when it constructs select queries to supply a default ordering to mapped objects.
* use_ansi=True : used only by Oracle;  when False, the Oracle driver attempts to support a particular "quirk" of some Oracle databases, that the LEFT OUTER JOIN SQL syntax is not supported, and the "Oracle join" syntax of using &lt;column1&gt;(+)=&lt;column2&gt; must be used in order to achieve a LEFT OUTER JOIN.  Its advised that the Oracle database be configured to have full ANSI support instead of using this feature.
* use_oids=False : used only by Postgres, will enable the column name "oid" as the object ID column.  Postgres as of 8.1 has object IDs disabled by default.
* convert_unicode=False : if set to True, all String/character based types will convert Unicode values to raw byte values going into the database, and all raw byte values to Python Unicode coming out in result sets.  This is an engine-wide method to provide unicode across the board.  For unicode conversion on a column-by-column level, use the Unicode column type instead.
* encoding='utf-8' : the encoding to use for Unicode translations - passed to all encode/decode methods.
* echo_uow=False : when True, logs unit of work commit plans to the standard output.

### Database Engine Methods {@name=methods}

A few useful methods off the SQLEngine are described here:

    {python}engine = create_engine('postgres://hostname=localhost&amp;user=scott&amp;password=tiger&amp;database=test')

    # get a pooled DBAPI connection
    conn = engine.connection()

    # create/drop tables based on table metadata objects
    # (see the next section, Table Metadata, for info on table metadata)
    engine.create(mytable)
    engine.drop(mytable)

    # get the DBAPI module being used
    dbapi = engine.dbapi()

    # get the default schema name
    name = engine.get_default_schema_name()

    # execute some SQL directly, returns a ResultProxy (see the SQL Construction section for details)
    result = engine.execute("select * from table where col1=:col1", {'col1':'foo'})

    # log a message to the engine's log stream
    engine.log('this is a message')
               
    
### Using the Proxy Engine {@name=proxy}

The ProxyEngine is useful for applications that need to swap engines
at runtime, or to create their tables and mappers before they know
what engine they will use. One use case is an application meant to be
pluggable into a mix of other applications, such as a WSGI
application. Well-behaved WSGI applications should be relocatable; and
since that means that two versions of the same application may be
running in the same process (or in the same thread at different
times), WSGI applications ought not to depend on module-level or
global configuration. Using the ProxyEngine allows a WSGI application
to define tables and mappers in a module, but keep the specific
database connection uri as an application instance or thread-local
value.

The ProxyEngine is used in the same way as any other engine, with one
additional method:
    
    {python}# define the tables and mappers
    from sqlalchemy import *
    from sqlalchemy.ext.proxy import ProxyEngine
    
    engine = ProxyEngine()
    
    users = Table('users', engine, ... )
    
    class Users(object):
        pass
        
    assign_mapper(Users, users)
    
    def app(environ, start_response):
        # later, connect the proxy engine to a real engine via the connect() method
        engine.connect(environ['db_uri'])
        # now you have a real db connection and can select, insert, etc.
    

#### Using the Global Proxy {@name=defaultproxy}
    
There is an instance of ProxyEngine available within the schema package as `default_engine`.  You can construct Table objects and not specify the engine parameter, and they will connect to this engine by default.  To connect the default_engine, use the `global_connect` function.

    {python}# define the tables and mappers
    from sqlalchemy import *
    
    # specify a table with no explicit engine
    users = Table('users', 
            Column('user_id', Integer, primary_key=True),
            Column('user_name', String)
        )
    
    # connect the global proxy engine
    global_connect('sqlite://filename=foo.db')
    
    # create the table in the selected database
    users.create()
    

### Transactions {@name=transactions}

A SQLEngine also provides an interface to the transactional capabilities of the underlying DBAPI connection object, as well as the connection object itself.  Note that when using the object-relational-mapping package, described in a later section, basic transactional operation is handled for you automatically by its "Unit of Work" system;  the methods described here will usually apply just to literal SQL update/delete/insert operations or those performed via the SQL construction library.
    
Typically, a connection is opened with `autocommit=False`.  So to perform SQL operations and just commit as you go, you can simply pull out a connection from the connection pool, keep it in the local scope, and call commit() on it as needed.  As long as the connection remains referenced, all other SQL operations within the same thread will use this same connection, including those used by the SQL construction system as well as the object-relational mapper, both described in later sections:

    {python}conn = engine.connection()
    
    # execute SQL via the engine
    engine.execute("insert into mytable values ('foo', 'bar')")
    conn.commit()
    
    # execute SQL via the SQL construction library            
    mytable.insert().execute(col1='bat', col2='lala')
    conn.commit()
        
There is a more automated way to do transactions, and that is to use the engine's begin()/commit() functionality.  When the begin() method is called off the engine, a connection is checked out from the pool and stored in a thread-local context.  That way, all subsequent SQL operations within the same thread will use that same connection.  Subsequent commit() or rollback() operations are performed against that same connection.  In effect, its a more automated way to perform the "commit as you go" example above.  
    
    {python}engine.begin()
    engine.execute("insert into mytable values ('foo', 'bar')")
    mytable.insert().execute(col1='foo', col2='bar')
    engine.commit()
        

A traditional "rollback on exception" pattern looks like this:    

    {python}engine.begin()
    try:
        engine.execute("insert into mytable values ('foo', 'bar')")
        mytable.insert().execute(col1='foo', col2='bar')
    except:
        engine.rollback()
        raise
    engine.commit()
    

An shortcut which is equivalent to the above is provided by the `transaction` method:
    
    {python}def do_stuff():
            engine.execute("insert into mytable values ('foo', 'bar')")
            mytable.insert().execute(col1='foo', col2='bar')

    engine.transaction(do_stuff)
        
An added bonus to the engine's transaction methods is "reentrant" functionality; once you call begin(), subsequent calls to begin() will increment a counter that must be decremented corresponding to each commit() statement before an actual commit can happen.  This way, any number of methods that want to insure a transaction can call begin/commit, and be nested arbitrarily:

    {python}# method_a starts a transaction and calls method_b
    def method_a():
        engine.begin()
        try:
            method_b()
        except:
            engine.rollback()
            raise
        engine.commit()

    # method_b starts a transaction, or joins the one already in progress,
    # and does some SQL
    def method_b():
        engine.begin()
        try:
            engine.execute("insert into mytable values ('bat', 'lala')")
            mytable.insert().execute(col1='bat', col2='lala')
        except:
            engine.rollback()
            raise
        engine.commit()
        
    # call method_a                
    method_a()                
            
Above, `method_a` is called first, which calls `engine.begin()`.  Then it calls `method_b`. When `method_b` calls `engine.begin()`, it just increments a counter that is decremented when it calls `commit()`.  If either `method_a` or `method_b` calls `rollback()`, the whole transaction is rolled back.  The transaction is not committed until `method_a` calls the `commit()` method.
       
The object-relational-mapper capability of SQLAlchemy includes its own `commit()` method that gathers SQL statements into a batch and runs them within one transaction.  That transaction is also invokved within the scope of the "reentrant" methodology above; so multiple objectstore.commit() operations can also be bundled into a larger database transaction via the above methodology.
    

